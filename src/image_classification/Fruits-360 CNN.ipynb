{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "##############################################\n",
    "learning_rate = 0.1  # initial learning rate\n",
    "min_learning_rate = 0.00001  # once the learning rate reaches this value, do not decrease it further\n",
    "learning_rate_reduction_factor = 0.5  # the factor used when reducing the learning rate -> learning_rate *= learning_rate_reduction_factor\n",
    "patience = 3  # how many epochs to wait before reducing the learning rate when the loss plateaus\n",
    "verbose = 1  # controls the amount of logging done during training and testing: 0 - none, 1 - reports metrics after each batch, 2 - reports metrics after each epoch\n",
    "image_size = (100, 100)  # width and height of the used images\n",
    "input_shape = (100, 100, 3)  # the expected input shape for the trained models; since the images in the Fruit-360 are 100 x 100 RGB images, this is the required input shape\n",
    "\n",
    "use_label_file = False  # set this to true if you want load the label names from a file; uses the label_file defined below; the file should contain the names of the used labels, each label on a separate line\n",
    "label_file = 'labels.txt'\n",
    "base_dir = '../..'  # relative path to the Fruit-Images-Dataset folder\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "train_dir = os.path.join(base_dir, 'Training')\n",
    "output_dir = 'output_files'  # root folder in which to save the the output files; the files will be under output_files/model_name \n",
    "##############################################\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if use_label_file:\n",
    "    with open(label_file, \"r\") as f:\n",
    "        labels = [x.strip() for x in f.readlines()]\n",
    "else:\n",
    "    labels = os.listdir(train_dir)\n",
    "num_classes = len(labels)\n",
    "\n",
    "# create 2 charts, one for accuracy, one for loss, to show the evolution of these two metrics during the training process\n",
    "def plot_model_history(model_history, out_path=\"\"):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1, len(model_history.history['acc']) + 1), model_history.history['acc'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_acc']) + 1), model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1, len(model_history.history['acc']) + 1), len(model_history.history['acc']))\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1, len(model_history.history['loss']) + 1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss']) + 1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1, len(model_history.history['loss']) + 1), len(model_history.history['loss']))\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    # save the graph in a file called \"acc_loss.png\" to be available for later; the model_name is provided when creating and training a model\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/acc_loss.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# create a confusion matrix to visually represent incorrectly classified images\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, out_path=\"\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=[i for i in classes], columns=[i for i in classes])\n",
    "    plt.figure(figsize=(40, 40))\n",
    "    ax = sn.heatmap(df_cm, annot=True, square=True, fmt=\"d\", linewidths=.2, cbar_kws={\"shrink\": 0.8})\n",
    "    if out_path:\n",
    "        plt.savefig(out_path + \"/confusion_matrix.png\")  # as in the plot_model_history, the matrix is saved in a file called \"model_name_confusion_matrix.png\"\n",
    "    return ax\n",
    "\n",
    "\n",
    "# given the train and test folder paths and a validation to test ratio, this method creates three generators\n",
    "#  - the training generator uses (100 - validation_percent) of images from the train set \n",
    "#    it applies random horizontal and vertical flips for data augmentation and generates batches randomly\n",
    "#  - the validation generator uses the remaining validation_percent of images from the train set\n",
    "#    does not generate random batches, as the model is not trained on this data\n",
    "#    the accuracy and loss are monitored using the validation data so that the learning rate can be updated if the model hits a local optimum\n",
    "#  - the test generator uses the test set without any form of augmentation\n",
    "#    once the training process is done, the final values of accuracy and loss are calculated on this set\n",
    "def build_data_generators(train_folder, test_folder, validation_percent, labels=None, image_size=(100, 100), batch_size=50):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=0.0,\n",
    "        height_shift_range=0.0,\n",
    "        zoom_range=0.0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,  # randomly flip images\n",
    "        validation_split=validation_percent)  # percentage indicating how much of the training set should be kept for validation\n",
    "\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(train_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                  batch_size=batch_size, shuffle=True, subset='training', classes=labels)\n",
    "    validation_gen = train_datagen.flow_from_directory(train_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                       batch_size=batch_size, shuffle=False, subset='validation', classes=labels)\n",
    "    test_gen = test_datagen.flow_from_directory(test_folder, target_size=image_size, class_mode='sparse',\n",
    "                                                batch_size=batch_size, shuffle=False, subset=None, classes=labels)\n",
    "    return train_gen, validation_gen, test_gen\n",
    "\n",
    "\n",
    "# this method performs all the steps from data setup, training and testing the model and plotting the results\n",
    "# the model is any trainable model; the input shape and output number of classes is dependant on the dataset used, in this case the input is 100x100 RGB images and the output is a softmax layer with 118 probabilities\n",
    "# the name is used to save the classification report containing the f1 score of the model, the plots showing the loss and accuracy and the confusion matrix\n",
    "# the batch size is used to determine the number of images passed through the network at once, the number of steps per epochs is derived from this as (total number of images in set // batch size) + 1\n",
    "def train_and_evaluate_model(model, name=\"\", epochs=25, batch_size=50, verbose=verbose, useCkpt=False):\n",
    "    print(model.summary())\n",
    "    model_out_dir = os.path.join(output_dir, name)\n",
    "    if not os.path.exists(model_out_dir):\n",
    "        os.makedirs(model_out_dir)\n",
    "    if useCkpt:\n",
    "        model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "    trainGen, validationGen, testGen = build_data_generators(train_dir, test_dir, validation_percent=0.1, labels=labels, image_size=image_size, batch_size=batch_size)\n",
    "    optimizer = Adadelta(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=patience, verbose=verbose, \n",
    "                                                factor=learning_rate_reduction_factor, min_lr=min_learning_rate)\n",
    "    save_model = ModelCheckpoint(filepath=model_out_dir + \"/model.h5\", monitor='val_acc', verbose=verbose, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "    \n",
    "    history = model.fit_generator(generator=trainGen,\n",
    "                                  epochs=epochs,\n",
    "                                  steps_per_epoch=(trainGen.n // batch_size) + 1,\n",
    "                                  validation_data=validationGen,\n",
    "                                  validation_steps=(validationGen.n // batch_size) + 1,\n",
    "                                  verbose=verbose,\n",
    "                                  callbacks=[learning_rate_reduction, save_model])\n",
    "\n",
    "    model.load_weights(model_out_dir + \"/model.h5\")\n",
    "\n",
    "    validationGen.reset()\n",
    "    loss_v, accuracy_v = model.evaluate_generator(validationGen, steps=(validationGen.n // batch_size) + 1, verbose=verbose)\n",
    "    loss, accuracy = model.evaluate_generator(testGen, steps=(testGen.n // batch_size) + 1, verbose=verbose)\n",
    "    print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
    "    print(\"Test: accuracy = %f  ;  loss_v = %f\" % (accuracy, loss))\n",
    "    plot_model_history(history, out_path=model_out_dir)\n",
    "    testGen.reset()\n",
    "    y_pred = model.predict_generator(testGen, steps=(testGen.n // batch_size) + 1, verbose=verbose)\n",
    "    y_true = testGen.classes[testGen.index_array]\n",
    "    plot_confusion_matrix(y_true, y_pred.argmax(axis=-1), labels, out_path=model_out_dir)\n",
    "    class_report = classification_report(y_true, y_pred.argmax(axis=-1), target_names=labels)\n",
    "\n",
    "    with open(model_out_dir + \"/classification_report.txt\", \"w\") as text_file:\n",
    "        text_file.write(\"%s\" % class_report)\n",
    "    # print(class_report)\n",
    "\n",
    "\n",
    "print(labels)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, Lambda\n",
    "\n",
    "\n",
    "# Create a custom layer that converts the original image from \n",
    "# RGB to HSV and grayscale and concatenates the results\n",
    "# forming in input of size 100 x 100 x 4 \n",
    "def image_process(x):\n",
    "    import tensorflow as tf\n",
    "    hsv = tf.image.rgb_to_hsv(x)\n",
    "    gray = tf.image.rgb_to_grayscale(x)\n",
    "    rez = tf.concat([hsv, gray], axis=-1)\n",
    "    return rez\n",
    "\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    img_input = Input(shape=input_shape, name='data')\n",
    "    x = Lambda(image_process)(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), padding='same', name='conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool1')(x)\n",
    "    x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', name='conv2')(x)\n",
    "    x = Activation('relu', name='conv2_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool2')(x)\n",
    "    x = Conv2D(64, (5, 5), strides=(1, 1), padding='same', name='conv3')(x)\n",
    "    x = Activation('relu', name='conv3_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool3')(x)\n",
    "    x = Conv2D(128, (5, 5), strides=(1, 1), padding='same', name='conv4')(x)\n",
    "    x = Activation('relu', name='conv4_relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='valid', name='pool4')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu', name='fcl1')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu', name='fcl2')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    rez = Model(inputs=img_input, outputs=out)\n",
    "    return rez\n",
    "\n",
    "\n",
    "model = network(input_shape=input_shape, num_classes=num_classes)\n",
    "train_and_evaluate_model(model, name=\"fruit-360 model\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fruits-360 CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}